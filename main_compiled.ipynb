{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5732f7fb",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2b67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching\n",
      "Driver closed.\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "# Scraping abstracts for years\n",
    "\n",
    "def driver_setup():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_experimental_option(\"prefs\", {\n",
    "        \"profile.managed_default_content_settings.images\": 2  # block images\n",
    "    })\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    capabilities = webdriver.DesiredCapabilities.CHROME.copy()\n",
    "    capabilities['pageLoadStrategy'] = 'eager'  # This ensures page load considers DOM ready only\n",
    "    # Launch the undetected driver with the capabilities and options\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.execute_cdp_cmd(\"Network.enable\", {})\n",
    "    driver.execute_cdp_cmd(\"Network.setBlockedURLs\", {\n",
    "        \"urls\": [\n",
    "            \"*.png\", \"*.jpg\", \"*.jpeg\", \"*.gif\", \"*.svg\",  # Images and stylesheets\n",
    "            \"*.woff\", \"*.woff2\", \"*.ttf\", \"*.ico\",  # Fonts\n",
    "            \"*.mp4\", \"*.webm\", \"*.avi\", \"*.mov\", \"*.mkv\",  # Videos\n",
    "            \"*.json\", \"*.xml\"  # Optional: Block large API responses if not required\n",
    "        ]\n",
    "    })\n",
    "    driver.execute_script(\"\"\"\n",
    "            var videos = document.querySelectorAll('video');\n",
    "            videos.forEach(function(video) {\n",
    "                video.autoplay = false;  // Disable autoplay\n",
    "                video.pause();           // Pause video if it's playing\n",
    "            });\n",
    "        \"\"\")\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "def article_scraper(driver, link):\n",
    "    try:\n",
    "        driver.get(link)\n",
    "        name = driver.title.strip()\n",
    "        print(f\"Processing site: {name}\")\n",
    "        wait = WebDriverWait(driver, 5)  # Timeout after 20 seconds\n",
    "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(\"Page Loaded\")\n",
    "\n",
    "        previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        scroll_attempts = 0  # Count the number of scroll attempts without content loading\n",
    "        load_clicks = 0\n",
    "        show_clicks = 0\n",
    "\n",
    "        while True:\n",
    "            articles = []\n",
    "            try:\n",
    "                articles = wait.until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article.mg-result-item.px-2.py-3.border-top\"\n",
    "                                                                          \".border-bottom\")))\n",
    "                print(f\"articles length = '{len(articles)}' \")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to locate articles: {e}\")\n",
    "\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    article_name = article.find_element(By.CSS_SELECTOR, \"a.no-underline\").text.strip()\n",
    "                    # show_abstract = WebDriverWait(article, 5).until(\n",
    "                    #     EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.bg-transparent.p-0.my-3.text-secondary\"\n",
    "                    #                                                  \".border-0.block.cursor-pointer\"))\n",
    "                    # )\n",
    "                    # driver.execute_script(\"arguments[0].click();\", show_abstract)\n",
    "                    show_clicks += 1\n",
    "\n",
    "                    # abstract_content = WebDriverWait(article, 5).until(\n",
    "                    #     EC.visibility_of_element_located((By.CSS_SELECTOR, \"div.mb-3\"))\n",
    "                    # )\n",
    "\n",
    "                    with open(\"abstracts.txt\", \"a\", encoding=\"utf-8\") as file:  # \"a\" mode ensures continuous writing\n",
    "                        file.write(f\"Article: {article_name}\\n\")\n",
    "                        # file.write(f\"Abstract: {abstract_content.text.strip()}\\n\\n\")  # Double line break for separation\n",
    "                    print(f\"Abstract saved for: {article_name}\")\n",
    "\n",
    "                except Exception:\n",
    "                    print(\"ERROR OCCURED WHILE PARSING ARTICLES\")\n",
    "                    break\n",
    "\n",
    "            print(f\"\\n articles passed = '{show_clicks}'\")\n",
    "\n",
    "            try:\n",
    "                load_next_button = WebDriverWait(driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.border-solid.relative.inline-flex.items-center\"\n",
    "                                                                 \".rounded-r-md.border.border-dividers.bg-white.px-2.py-2.font-medium\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_next_button)\n",
    "                driver.execute_script(\"arguments[0].click();\", load_next_button)\n",
    "                time.sleep(2)\n",
    "                load_clicks += 1\n",
    "                scroll_attempts = 0  # Reset scroll attempts after a successful load\n",
    "            except Exception:\n",
    "                print(\"'Load More' button not found or no more content to load.\")\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                scroll_attempts += 1\n",
    "\n",
    "            if scroll_attempts >= 1:  # After a few failed scrolls, stop\n",
    "                print(\"No more new content loaded after several attempts. Stopping.\")\n",
    "                break\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Error fetching\")\n",
    "\n",
    "# The below is code for directly scraping keywords for 2019 as its abstract data was not available\n",
    "# def article_scraper(driver, link):\n",
    "#     try:\n",
    "#         driver.get(link)\n",
    "#         name = driver.title.strip()\n",
    "#         print(f\"Processing site: {name}\")\n",
    "#         wait = WebDriverWait(driver, 5)\n",
    "#         wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "#         print(\"Page Loaded\")\n",
    "\n",
    "#         while True:\n",
    "#             articles = driver.find_elements(By.CSS_SELECTOR, \"article.mg-result-item\")\n",
    "#             print(f\"Found {len(articles)} articles\")\n",
    "\n",
    "#             for article in articles:\n",
    "#                 try:\n",
    "#                     keywords_container = article.find_elements(By.CSS_SELECTOR, \"div.text-gray-light span a\")\n",
    "#                     keywords = [keyword.text.strip() for keyword in keywords_container]\n",
    "#                     if keywords:\n",
    "#                         with open(\"keywords.txt\", \"a\", encoding=\"utf-8\") as file:\n",
    "#                             file.write(f\"Keywords: {', '.join(keywords)}\\n\")\n",
    "#                         print(f\"Keywords saved: {', '.join(keywords)}\")\n",
    "#                 except Exception:\n",
    "#                     print(\"Error occurred while extracting keywords\")\n",
    "\n",
    "#             try:\n",
    "#                 load_next_button = WebDriverWait(driver, 5).until(\n",
    "#                     EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.border-solid.relative.inline-flex.items-center.rounded-r-md\"))\n",
    "#                 )\n",
    "#                 driver.execute_script(\"arguments[0].click();\", load_next_button)\n",
    "#                 time.sleep(2)\n",
    "#             except Exception:\n",
    "#                 print(\"'Load More' button not found or no more content to load.\")\n",
    "#                 break\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching: {e}\")\n",
    "\n",
    "def main():\n",
    "    driver = driver_setup()\n",
    "    link = r\"https://www.mrs.org/meetings-events/annual-meetings/archive/meeting/presentations/2022-mrs-spring-meeting?page\" \\\n",
    "           r\"=1&categories=&symposium=&sessiontype=&topicalcluster=&sessiondate=\"\n",
    "    try:\n",
    "        article_scraper(driver, link)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Driver closed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265d8c4",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_conference_text(text):\n",
    "    # Split the text into individual lines\n",
    "    lines = text.split('\\n')\n",
    "    cleaned = []\n",
    "    current_line = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        # Check if the line is empty (after stripping whitespace)\n",
    "        if not stripped:\n",
    "            # If there's content in current_line, join and add to cleaned\n",
    "            if current_line:\n",
    "                cleaned.append(' '.join(current_line))\n",
    "                current_line = []\n",
    "            # Add an empty line to preserve the line break\n",
    "            cleaned.append('')\n",
    "        else:\n",
    "            # Add the stripped line to the current paragraph\n",
    "            current_line.append(stripped)\n",
    "    \n",
    "    # Add any remaining content in current_line\n",
    "    if current_line:\n",
    "        cleaned.append(' '.join(current_line))\n",
    "    \n",
    "    # Join the cleaned lines, replacing empty strings with actual newlines\n",
    "    result = '\\n'.join(cleaned)\n",
    "    # Remove excessive empty lines (optional, adjust as needed)\n",
    "    result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
    "\n",
    "    return result\n",
    "\n",
    "def clean_text(text):\n",
    "    # Step 1: Replace single line breaks (but keep double line breaks for paragraphs)\n",
    "    # text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "\n",
    "    # Step 2: Remove unwanted session metadata\n",
    "    # text = re.sub(r\"Final Program\\s*–\\s*\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}\", \"\", text)\n",
    "    # text = re.sub(r\"SESSION\\s+[A-Z]+\\d+\\.\\d+\\s*:\\s*.*\", \"\", text)\n",
    "    text = re.sub(r\"Session Chairs?:\\s*.*\", \"\", text)\n",
    "    text = re.sub(r\"[A-Za-z]+day\\s*(Morning|Afternoon|Evening),?\\s*[A-Za-z]+\\s+\\d{1,2},\\s*\\d{4}\", \"\", text)\n",
    "\n",
    "    # Step 3: Remove extra spaces\n",
    "    # text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def merge_lines(text):\n",
    "    pattern = re.compile(r'([-,])([^\\S\\n]*)\\n+([^\\S\\n]*)', flags=re.MULTILINE)\n",
    "    return pattern.sub(r'\\1\\2\\3', text)\n",
    "\n",
    "def process_text(text):\n",
    "    # Remove lines that start with 'Acknowledgement', 'References' or '[1]'\n",
    "    text = re.sub(r'(?m)^(Acknowledgement|References|\\[\\d+\\]).*$', '', text)\n",
    "\n",
    "    # Split text into lines and process them\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    merged_lines = []\n",
    "    current_para = \"\"\n",
    "\n",
    "    time_pattern = re.compile(r\"^\\d{1,2}:\\d{2} [APM]{2}\")  # Match time format\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # Skip blank lines\n",
    "        \n",
    "        if time_pattern.match(line):  # New paragraph starts\n",
    "            if current_para:\n",
    "                merged_lines.append(current_para)  # Store the previous paragraph\n",
    "            current_para = line  # Start a new paragraph\n",
    "        else:\n",
    "            current_para += \" \" + line  # Append to the current paragraph\n",
    "        \n",
    "    if current_para:\n",
    "        merged_lines.append(current_para)  # Add the last paragraph\n",
    "\n",
    "    return \"\\n\\n\".join(merged_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2020-mrs-fall-meeting-abstracts.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# cleaned_text = clean_conference_text(raw_text)\n",
    "# cleaned_text = clean_text(raw_text)\n",
    "cleaned_text = process_text(raw_text)\n",
    "# cleaned_text = merge_lines(cleaned_text)\n",
    "# Save the cleaned text\n",
    "with open(\"2020-spring-fall-final.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(cleaned_text)\n",
    "\n",
    "print(\"Text cleaning complete. Saved as 'cleaned_conference_text.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889d9af",
   "metadata": {},
   "source": [
    "# Keywords Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10793e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from krutrim_cloud import KrutrimCloud\n",
    "\n",
    "# Initialize API client\n",
    "client = KrutrimCloud(api_key='')  # use your api key\n",
    "\n",
    "# File paths\n",
    "fall_abstracts_path = 'Fall_Abstracts(2011-2018).txt'\n",
    "spring_abstracts_path = 'Spring_Abstracts(2011-2018).txt'\n",
    "output_json_path = 'abstracts_with_5_keywords.json'\n",
    "checkpoint_path = 'processed_abstracts_checkpoint.json'\n",
    "\n",
    "# Function to extract keywords\n",
    "def extract_keywords(abstract):\n",
    "    model_name = \"Llama-3.3-70B-Instruct\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert keyword extractor specialized in materials science and engineering. Your task is to analyze abstracts from the Materials Research Society (MRS) conferences and extract exactly 5 concise and specific keywords or phrases from each abstract. These keywords should: 1. Reflect the core topics, materials, techniques, or methods discussed in the abstract. 2. Be specific to the research focus of the abstract (e.g., 'perovskite solar cells' instead of 'solar cells'). 3. Avoid overly generic terms (e.g., 'materials science' or 'analysis'). 4. Be formatted as a comma-separated list without any additional text or explanation.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Abstract: \" + abstract + \" Extract exactly 5 concise and specific keywords or phrases that capture the core topics, techniques, materials, or methods discussed in this abstract. Ensure the keywords are relevant, specific, and formatted as a comma-separated list without any additional text or explanation. Output format: Keyword1, Keyword2, Keyword3, Keyword4, Keyword5\",\n",
    "        },\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(model=model_name, messages=messages)\n",
    "        return response.choices[0].message.content  # type: ignore\n",
    "    except Exception as exc:\n",
    "        print(f\"API Error: {exc}\")\n",
    "        return None\n",
    "\n",
    "# Process a single abstract\n",
    "def process_abstract(line, current_year):\n",
    "    try:\n",
    "        # Extract country as the last word before \"Show Abstract\"\n",
    "        pre_abstract_text = line.split(\"Show Abstract\")[0]\n",
    "        country_match = re.search(r\"(\\b\\w+)$\", pre_abstract_text.strip())\n",
    "        country = country_match.group(1) if country_match else \"Unknown Country\"\n",
    "\n",
    "        # Extract keywords\n",
    "        keywords = extract_keywords(line)\n",
    "        if not keywords:\n",
    "            print(f\"Failed to extract keywords for line: {line}\")\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"year\": current_year,\n",
    "            \"country\": country,\n",
    "            \"keywords\": [keyword.strip() for keyword in keywords.split(\", \")],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {line}\\n{e}\")\n",
    "        return None\n",
    "\n",
    "# Process the file\n",
    "def process_abstract_file(file_path, checkpoint=None):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.readlines()\n",
    "\n",
    "    current_year = None\n",
    "    processed_data = checkpoint or []\n",
    "    processed_lines = set(item[\"line_number\"] for item in processed_data)\n",
    "\n",
    "    year_pattern = re.compile(r\"(\\d{4}) Symposiums\")\n",
    "\n",
    "    for line_number, line in enumerate(content):\n",
    "        if line_number in processed_lines:\n",
    "            continue\n",
    "\n",
    "        # Detect the year\n",
    "        year_match = year_pattern.match(line.strip())\n",
    "        if year_match:\n",
    "            current_year = int(year_match.group(1))\n",
    "            continue\n",
    "\n",
    "        # Process abstracts\n",
    "        if \"Show Abstract\" in line and current_year:\n",
    "            print(f\"Processing line {line_number} for year {current_year}\")\n",
    "            result = process_abstract(line, current_year)\n",
    "            if result:\n",
    "                result[\"line_number\"] = line_number\n",
    "                processed_data.append(result)\n",
    "\n",
    "            # Save checkpoint\n",
    "            if len(processed_data) % 100 == 0:\n",
    "                with open(checkpoint_path, 'w') as checkpoint_file:\n",
    "                    json.dump(processed_data, checkpoint_file, indent=4)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Load checkpoint\n",
    "try:\n",
    "    with open(checkpoint_path, 'r') as checkpoint_file:\n",
    "        checkpoint_data = json.load(checkpoint_file)\n",
    "except FileNotFoundError:\n",
    "    checkpoint_data = []\n",
    "\n",
    "# Process files\n",
    "fall_data = process_abstract_file(fall_abstracts_path, checkpoint_data)\n",
    "spring_data = process_abstract_file(spring_abstracts_path, fall_data)\n",
    "\n",
    "# Save final results\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(spring_data, json_file, indent=4)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7860d",
   "metadata": {},
   "source": [
    "# Combining 4 different years kaywords json files to one for umap + fastcluster parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def combine_json_files(root_folder, combined_json_file):\n",
    "    combined_data = []\n",
    "    count=0\n",
    "    for filename in os.listdir(root_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(root_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                try:\n",
    "                    data = json.load(file)\n",
    "                    for entry in data:\n",
    "                        if \"year\" in entry and \"keywords\" in entry and entry['year'] in [2011, 2015, 2020, 2024]:\n",
    "                            combined_data.append({\n",
    "                                \"year\": entry[\"year\"],\n",
    "                                \"keywords\": entry[\"keywords\"]\n",
    "                            })\n",
    "                            count+=len(entry[\"keywords\"])\n",
    "                        else:\n",
    "                            print(f\"Skipping entry without 'year' or 'keywords': {entry}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON file: {filename}\")\n",
    "    print(f\"Total keywords combined: {count}\")\n",
    "    combined_data.sort(key=lambda x: x[\"year\"])\n",
    "    \n",
    "    with open(combined_json_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(combined_data, outfile, indent=4)\n",
    "\n",
    "combine_json_files(\".\", \"combined_for_param_tuning.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729091f8",
   "metadata": {},
   "source": [
    "# Generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load JSON and extract keywords for a given year\n",
    "def load_keywords_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    keywords = []\n",
    "    for entry in data:\n",
    "        keywords.extend(entry['keywords'])\n",
    "    return list(set(keywords)) \n",
    "\n",
    "# Load Sentence-BERT model with GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "keywords = load_keywords_from_json('combined_for_param_tuning.json')\n",
    "dataloader = DataLoader(keywords, batch_size=1024, shuffle=False)\n",
    "embeddings = []\n",
    "for batch in dataloader:\n",
    "    batch_embeddings = model.encode(batch, show_progress_bar=True)\n",
    "    embeddings.append(batch_embeddings)\n",
    "    \n",
    "embeddings = np.vstack(embeddings)\n",
    "np.save('embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5592b13",
   "metadata": {},
   "source": [
    "# Parameter tuning (sample code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe122b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "print(embeddings.shape)  # Should be (n, 384)\n",
    "\n",
    "\n",
    "# Define expanded parameter combinations\n",
    "n_components_range = range(10, 101, 10)  # 10, 20, 30, ..., 100\n",
    "n_neighbors_range = range(10, 101, 10)  \n",
    "min_dist_values = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.65, 0.8, 0.99]\n",
    "distance_thresholds = [1.0, 1.5, 2.0, 2.5, 3.0]\n",
    "\n",
    "# File to track progress\n",
    "progress_file = \"clustering_progress.json\"\n",
    "results_file = \"clustering_results.csv\"\n",
    "\n",
    "# Generate all parameter combinations\n",
    "umap_params = []\n",
    "for n_comp in n_components_range:\n",
    "    for n_neigh in n_neighbors_range:\n",
    "        for min_dist in min_dist_values:\n",
    "            umap_params.append((n_comp, n_neigh, min_dist))\n",
    "\n",
    "# Load existing results and progress if available\n",
    "results = []\n",
    "completed_params = set()\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    df_results = pd.read_csv(results_file)\n",
    "    results = df_results.to_dict('records')\n",
    "    \n",
    "    # Extract already completed parameter combinations\n",
    "    for row in results:\n",
    "        param_key = (row[\"n_components\"], row[\"n_neighbors\"], row[\"min_dist\"], row[\"distance_threshold\"])\n",
    "        completed_params.add(param_key)\n",
    "    \n",
    "    print(f\"Loaded {len(results)} existing results from {results_file}\")\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as f:\n",
    "        progress_data = json.load(f)\n",
    "        last_index = progress_data.get('last_index', 0)\n",
    "        print(f\"Resuming from parameter combination index {last_index}\")\n",
    "else:\n",
    "    last_index = 0\n",
    "\n",
    "# Process remaining parameter combinations\n",
    "try:\n",
    "    for param_idx, (n_comp, n_neigh, min_dist) in enumerate(umap_params[last_index:], start=last_index):\n",
    "        print(f\"Processing UMAP: n_components={n_comp}, n_neighbors={n_neigh}, min_dist={min_dist} [{param_idx+1}/{len(umap_params)}]\")\n",
    "        \n",
    "        # Apply UMAP\n",
    "        umap_reducer = umap.UMAP(\n",
    "            n_components=n_comp, \n",
    "            n_neighbors=n_neigh, \n",
    "            min_dist=min_dist, \n",
    "            metric='cosine', \n",
    "            random_state=42\n",
    "        )\n",
    "        umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "        \n",
    "        for dist_thresh in distance_thresholds:\n",
    "            # Skip if this combination has already been processed\n",
    "            param_key = (n_comp, n_neigh, min_dist, dist_thresh)\n",
    "            if param_key in completed_params:\n",
    "                print(f\"  - Skipping AgglomerativeClustering with distance_threshold={dist_thresh} (already processed)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  - Applying AgglomerativeClustering with distance_threshold={dist_thresh}\")\n",
    "            \n",
    "            # Apply Fast Clustering\n",
    "            linkage_matrix = fastcluster.linkage_vector(umap_embeddings, method='ward')\n",
    "            labels = fcluster(linkage_matrix, t=dist_thresh, criterion='distance')\n",
    "            num_clusters = len(set(labels))\n",
    "            \n",
    "            # Compute silhouette score only if there is more than 1 cluster and more than 1 sample\n",
    "            if num_clusters > 1 and len(set(labels)) < len(labels):\n",
    "                try:\n",
    "                    sil_score = silhouette_score(umap_embeddings, labels)\n",
    "                except:\n",
    "                    sil_score = -1  # Error in calculation\n",
    "            else:\n",
    "                sil_score = -1  # Invalid case (all points in one cluster)\n",
    "            \n",
    "            # Save cluster assignments to JSON\n",
    "            clustered_keywords = {}\n",
    "            for keyword, cluster in zip(keywords, labels):\n",
    "                clustered_keywords.setdefault(str(cluster), []).append(keyword)\n",
    "            \n",
    "            filename = f\"clustered_keywords_ncomp{n_comp}_nneigh{n_neigh}_mindist{min_dist}_dthresh{dist_thresh}.json\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(clustered_keywords, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"    - Saved results to {filename}\")\n",
    "            \n",
    "            # Store results in table\n",
    "            results.append({\n",
    "                \"n_components\": n_comp,\n",
    "                \"n_neighbors\": n_neigh,\n",
    "                \"min_dist\": min_dist,\n",
    "                \"distance_threshold\": dist_thresh,\n",
    "                \"num_clusters\": num_clusters,\n",
    "                \"silhouette_score\": sil_score\n",
    "            })\n",
    "            \n",
    "            # Save results incrementally\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_csv(results_file, index=False)\n",
    "            \n",
    "            # Update progress\n",
    "            with open(progress_file, 'w') as f:\n",
    "                json.dump({'last_index': param_idx}, f)\n",
    "            \n",
    "            completed_params.add(param_key)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProcess interrupted by user. Progress has been saved.\")\n",
    "\n",
    "print(f\"\\nClustering completed/paused. Results saved in '{results_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000a000",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_graphs(df):\n",
    "    # Plot silhouette_score vs other parameters\n",
    "    params = ['n_components', 'n_neighbors', 'min_dist', 'distance_threshold', 'num_clusters']\n",
    "    for param in params:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.scatterplot(x=df[param], y=df[df['silhouette_score']>0]['silhouette_score'])\n",
    "        plt.xlabel(param)\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.title(f'Silhouette Score vs {param}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot num_clusters vs other parameters\n",
    "    for param in params[:-1]:  # Excluding num_clusters itself\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.scatterplot(x=df[param], y=df['num_clusters'])\n",
    "        plt.xlabel(param)\n",
    "        plt.ylabel('Number of Clusters')\n",
    "        plt.title(f'Number of Clusters vs {param}')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "df_sorted = pd.read_csv(\"clustering_results.csv\")\n",
    "plot_graphs(df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd05add",
   "metadata": {},
   "source": [
    "# Combining all keywords json files to apply umap and fastcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def combine_json_files(root_folder, combined_json_file):\n",
    "    combined_data = []\n",
    "    count=0\n",
    "    for filename in os.listdir(root_folder):\n",
    "        if filename.endswith(\".json\") and filename != \"combined_for_param_tuning.json\":\n",
    "            file_path = os.path.join(root_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                try:\n",
    "                    data = json.load(file)\n",
    "                    for entry in data:\n",
    "                        if \"year\" in entry and \"keywords\" in entry:\n",
    "                            combined_data.append({\n",
    "                                \"year\": entry[\"year\"],\n",
    "                                \"keywords\": entry[\"keywords\"]\n",
    "                            })\n",
    "                            count+=len(entry[\"keywords\"])\n",
    "                        else:\n",
    "                            print(f\"Skipping entry without 'year' or 'keywords': {entry}\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid JSON file: {filename}\")\n",
    "    print(f\"Total keywords combined: {count}\")\n",
    "    combined_data.sort(key=lambda x: x[\"year\"])\n",
    "    \n",
    "    with open(combined_json_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(combined_data, outfile, indent=4)\n",
    "\n",
    "# Example usage\n",
    "combine_json_files(\".\", \"combined.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a923f",
   "metadata": {},
   "source": [
    "# Applying Umap + Fastcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90454209",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load JSON and extract keywords for a given year\n",
    "def load_keywords_from_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    keywords = []\n",
    "    for entry in data:\n",
    "        keywords.extend(entry['keywords'])\n",
    "    return list(set(keywords)) \n",
    "\n",
    "# Load Sentence-BERT model with GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "keywords = load_keywords_from_json('combined.json')\n",
    "dataloader = DataLoader(keywords, batch_size=1024, shuffle=False)\n",
    "embeddings = []\n",
    "for batch in dataloader:\n",
    "    batch_embeddings = model.encode(batch, show_progress_bar=True)\n",
    "    embeddings.append(batch_embeddings)\n",
    "    \n",
    "embeddings = np.vstack(embeddings)\n",
    "np.save('embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "from cuml.manifold import UMAP as cumlUMAP\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from itertools import product\n",
    "\n",
    "# === Config ===\n",
    "INPUT_JSON = \"combined.json\"\n",
    "OUTPUT_DIR = \"./cluster_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# UMAP and clustering params\n",
    "N_COMPONENTS = [40,50]\n",
    "N_NEIGHBORS = [10, 20]\n",
    "MIN_DISTS = [0.01, 0.05, 0.2]\n",
    "DIST_THRESHOLDS = [0.5, 1, 1.5]\n",
    "\n",
    "umap_params = []\n",
    "for n_comp in N_COMPONENTS:\n",
    "    for n_neigh in N_NEIGHBORS:\n",
    "        for min_dist in MIN_DISTS:\n",
    "            umap_params.append((n_comp, n_neigh, min_dist))\n",
    "\n",
    "results_file = \"clustering_results.csv\"\n",
    "results=[]\n",
    "# Generate all parameter combinations\n",
    "umap_params = []\n",
    "for n_comp in n_components_range:\n",
    "    for n_neigh in n_neighbors_range:\n",
    "        for min_dist in min_dist_values:\n",
    "            umap_params.append((n_comp, n_neigh, min_dist))\n",
    "\n",
    "\n",
    "for param_idx, (n_comp, n_neigh, min_dist) in enumerate(umap_params[last_index:], start=last_index):\n",
    "    print(f\"Processing UMAP: n_components={n_comp}, n_neighbors={n_neigh}, min_dist={min_dist} [{param_idx+1}/{len(umap_params)}]\")\n",
    "    \n",
    "    # Apply UMAP\n",
    "    umap_reducer = umap.UMAP(\n",
    "        n_components=n_comp, \n",
    "        n_neighbors=n_neigh, \n",
    "        min_dist=min_dist, \n",
    "        metric='cosine', \n",
    "        random_state=42\n",
    "    )\n",
    "    umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "    \n",
    "    for dist_thresh in distance_thresholds:\n",
    "        \n",
    "        linkage_matrix = fastcluster.linkage_vector(umap_embeddings, method='ward')\n",
    "        labels = fcluster(linkage_matrix, t=dist_thresh, criterion='distance')\n",
    "        num_clusters = len(set(labels))\n",
    "        \n",
    "        # Compute silhouette score only if there is more than 1 cluster and more than 1 sample\n",
    "        if num_clusters > 1 and len(set(labels)) < len(labels):\n",
    "            try:\n",
    "                sil_score = silhouette_score(umap_embeddings, labels)\n",
    "            except:\n",
    "                sil_score = -1  # Error in calculation\n",
    "        else:\n",
    "            sil_score = -1  # Invalid case (all points in one cluster)\n",
    "        \n",
    "        # Save cluster assignments to JSON\n",
    "        clustered_keywords = {}\n",
    "        for keyword, cluster in zip(keywords, labels):\n",
    "            clustered_keywords.setdefault(str(cluster), []).append(keyword)\n",
    "        \n",
    "        filename = f\"clustered_keywords_ncomp{n_comp}_nneigh{n_neigh}_mindist{min_dist}_dthresh{dist_thresh}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(clustered_keywords, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"    - Saved results to {filename}\")\n",
    "        \n",
    "        # Store results in table\n",
    "        results.append({\n",
    "            \"n_components\": n_comp,\n",
    "            \"n_neighbors\": n_neigh,\n",
    "            \"min_dist\": min_dist,\n",
    "            \"distance_threshold\": dist_thresh,\n",
    "            \"num_clusters\": num_clusters,\n",
    "            \"silhouette_score\": sil_score\n",
    "        })\n",
    "        \n",
    "        # Save results incrementally\n",
    "        df_results = pd.DataFrame(results)\n",
    "        df_results.to_csv(results_file, index=False)\n",
    "    \n",
    "\n",
    "print(f\"\\nClustering completed/paused. Results saved in '{results_file}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c8451c",
   "metadata": {},
   "source": [
    "### The results of clustering on combined keywords of all years were bad due to UMAP, so Now trying to skip UMAPand and apply only fastcluster with dist_thresh=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0856ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set output directory\n",
    "OUTPUT_DIR = \"/clustering_results_new\"\n",
    "results_file = os.path.join(OUTPUT_DIR, \"clustering_results_without_umap.csv\")\n",
    "\n",
    "# Load JSON and extract keywords for a given year\n",
    "def load_keywords_from_json(file_path, target_year):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    keywords = []\n",
    "    for entry in data:\n",
    "        if entry['year']==target_year:\n",
    "            keywords.extend(entry['keywords'])\n",
    "    return list(set(keywords)) \n",
    "\n",
    "# Load Sentence-BERT model with GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "for year in range(2011,2025):\n",
    "    keywords = load_keywords_from_json('combined.json', target_year=year)\n",
    "    dataloader = DataLoader(keywords, batch_size=1024, shuffle=False)\n",
    "    embeddings = []\n",
    "    for batch in dataloader:\n",
    "        batch_embeddings = model.encode(batch, show_progress_bar=True)\n",
    "        embeddings.append(batch_embeddings)\n",
    "        \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    dist_thresh=1.0\n",
    "    print(f\"  - Applying AgglomerativeClustering with distance_threshold={dist_thresh}\")\n",
    "    linkage_matrix = fastcluster.linkage_vector(embeddings, method=\"ward\")\n",
    "    labels = fcluster(linkage_matrix, t=dist_thresh, criterion=\"distance\")\n",
    "    num_clusters = len(set(labels))\n",
    "    \n",
    "    # Compute silhouette score\n",
    "    sil_score = silhouette_score(embeddings, labels) if num_clusters > 1 else -1\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    clustered_keywords = {}\n",
    "    for keyword, cluster in zip(keywords, labels):\n",
    "        clustered_keywords.setdefault(str(cluster), []).append(keyword)\n",
    "    \n",
    "    filename = f\"{year}_fastclustr_dthresh{dist_thresh}.json\"\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clustered_keywords, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"    - Saved results to {filepath}\")\n",
    "    \n",
    "    results.append({\n",
    "        \"distance_threshold\": dist_thresh,\n",
    "        \"num_clusters\": num_clusters,\n",
    "        \"silhouette_score\": sil_score,\n",
    "        \"cluster_file\": filename\n",
    "    })\n",
    "    \n",
    "    # Save results incrementally\n",
    "    pd.DataFrame(results).to_csv(results_file, index=False)\n",
    "\n",
    "print(f\"\\nClustering completed. Results saved in '{results_file}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc02d9c",
   "metadata": {},
   "source": [
    "# Cluster Labelling using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from krutrim_cloud import KrutrimCloud\n",
    "api_key=\"\" # Use your api key\n",
    "# Initialize API client\n",
    "client = KrutrimCloud(api_key=api key)\n",
    "\n",
    "# Directory containing clustered keyword JSON files\n",
    "INPUT_DIR = \"./clustering_results_new\"  # Adjust if needed\n",
    "OUTPUT_DIR = \"./cluster_labels\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Years you have cluster files for\n",
    "YEARS = list(range(2011, 2025))  # 2011 to 2024\n",
    "\n",
    "# LLM model config\n",
    "MODEL_NAME = \"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Function to get representative label from LLM\n",
    "def label_cluster_with_llm(cluster_keywords):\n",
    "    keywords_text = \", \".join(cluster_keywords)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert in materials science. Your task is to assign a single representative keyword or phrase \"\n",
    "                \"to a given list of related materials science research keywords. Choose a concise, specific phrase that \"\n",
    "                \"accurately summarizes the entire cluster. If the keywords are too unrelated or vary widely, return only: None.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Choose a single short phrase or keyword (no more than 4-5 words).\\n\"\n",
    "                \"2. Make it specific and relevant (e.g., 'Mott-Schottky analysis', not 'materials characterization').\\n\"\n",
    "                \"3. Output only the label or 'None' with no additional explanation.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Cluster: {keywords_text}\\n\\nOutput a single label or 'None':\"\n",
    "        },\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(model=MODEL_NAME, messages=messages)\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API Error: {e}\")\n",
    "        return \"None\"\n",
    "\n",
    "# Process each year's cluster file\n",
    "for year in YEARS:\n",
    "    cluster_file = os.path.join(INPUT_DIR, f\"{year}_fastclust_dt1.json\")  # Adjust filename format if needed\n",
    "    if not os.path.exists(cluster_file):\n",
    "        print(f\"❌ Missing: {cluster_file}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n📦 Processing clusters from: {year}\")\n",
    "    with open(cluster_file, 'r', encoding='utf-8') as f:\n",
    "        clusters = json.load(f)\n",
    "\n",
    "    labeled_clusters = {}\n",
    "    for cluster_id, keywords in clusters.items():\n",
    "        # print(f\"→ Cluster {cluster_id} ({len(keywords)} keywords)\")\n",
    "        label = label_cluster_with_llm(keywords)\n",
    "        # print(f\"   🏷️ Label: {label}\")\n",
    "        labeled_clusters[cluster_id] = label\n",
    "\n",
    "    # Save labeled output\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{year}_labeled_clusters.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(labeled_clusters, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n✅ Done labeling all clusters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2095c",
   "metadata": {},
   "source": [
    "# Making a combined json file with all cluster labels and keywords count in that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23406e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Directory where the per-year JSONs are stored\n",
    "INPUT_DIR1 = \"./clustering_results_new\"\n",
    "INPUT_DIR2 = \"./cluster_labels\"  # Adjust if needed\n",
    "OUTPUT_JSON = \"keyword_timeline.json\"\n",
    "\n",
    "# Years to process\n",
    "YEARS = list(range(2011, 2025))\n",
    "\n",
    "# Output dictionary\n",
    "label_year_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for year in YEARS:\n",
    "    cluster_path = os.path.join(INPUT_DIR1, f\"{year}_fastclust_dt1.json\")\n",
    "    label_path = os.path.join(INPUT_DIR2, f\"{year}_labeled_clusters.json\")\n",
    "\n",
    "    if not os.path.exists(cluster_path) or not os.path.exists(label_path):\n",
    "        print(f\"Skipping {year} (files not found)\")\n",
    "        continue\n",
    "\n",
    "    with open(cluster_path, \"r\", encoding='utf-8') as f:\n",
    "        cluster_data = json.load(f)\n",
    "\n",
    "    with open(label_path, \"r\", encoding='utf-8') as f:\n",
    "        label_data = json.load(f)\n",
    "\n",
    "    for cluster_id, keywords in cluster_data.items():\n",
    "        label = label_data.get(cluster_id)\n",
    "        if label is not None:\n",
    "            label_year_counts[label][str(year)] += len(keywords)\n",
    "\n",
    "# Convert defaultdict to normal dict for JSON output\n",
    "final_result = {label: dict(years) for label, years in label_year_counts.items()}\n",
    "\n",
    "# Save to JSON\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Saved keyword label year-count data to {OUTPUT_JSON}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb142b6",
   "metadata": {},
   "source": [
    "#  Clustering on the combined labels file for generating 3 variants of final keywords sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c788c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import fastcluster\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "INPUT_JSON = \"keyword_timeline.json\"\n",
    "OUTPUT_DIR = \"./\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "DIST_THRESHOLDS = [0.5, 1, 1.5]\n",
    "\n",
    "# === Load model ===\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "\n",
    "# === Load keyword -> [years] mapping ===\n",
    "with open(INPUT_JSON, 'r', encoding='utf-8') as f:\n",
    "    keyword_to_years = json.load(f)\n",
    "\n",
    "keywords = list(keyword_to_years.keys())\n",
    "print(f\"Loaded {len(keywords)} unified keywords\")\n",
    "\n",
    "# === Embed keywords ===\n",
    "print(\"Encoding keywords...\")\n",
    "dataloader = DataLoader(keywords, batch_size=1024, shuffle=False)\n",
    "all_embeddings = []\n",
    "for batch in dataloader:\n",
    "    batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
    "    all_embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(all_embeddings).astype(np.float32)\n",
    "\n",
    "all_summary=[]\n",
    "for dist in DIST_THRESHOLDS:\n",
    "    # === Fastcluster clustering ===\n",
    "    print(f\"\\n🔧 Fastcluster(thresh={dist})\")\n",
    "    linkage_matrix = fastcluster.linkage_vector(embeddings, method='ward')\n",
    "    labels = fcluster(linkage_matrix, t=dist, criterion='distance')\n",
    "    n_clusters = len(set(labels))\n",
    "    \n",
    "    # Silhouette score\n",
    "    silhouette = silhouette_score(embeddings, labels) if n_clusters > 1 else -1\n",
    "    \n",
    "    # === Create cluster structure with aggregated year counts ===\n",
    "    clustered_keywords = defaultdict(list)\n",
    "    for kw, label in zip(keywords, labels):\n",
    "        clustered_keywords[f\"cluster_{label}\"].append(kw)\n",
    "    \n",
    "    # Final output: cluster_label -> {\"keywords\": [...], \"years\": {year: count, ...}}\n",
    "    clusters = {}\n",
    "    for cluster_id, kws in clustered_keywords.items():\n",
    "        year_counts = defaultdict(int)\n",
    "        for kw in kws:\n",
    "            for year, count in keyword_to_years[kw].items():\n",
    "                year_counts[year] += count\n",
    "    \n",
    "        clusters[cluster_id] = {\n",
    "            \"keywords\": kws,\n",
    "            \"years\": dict(sorted(year_counts.items()))\n",
    "        }\n",
    "    \n",
    "    # Save cluster JSON\n",
    "    filename_json = f\"clusters_dt{dist}_with_year_counts.json\"\n",
    "    with open(os.path.join(OUTPUT_DIR, filename_json), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clusters, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Log results\n",
    "    summary = [{\n",
    "        \"distance_threshold\": dist,\n",
    "        \"n_clusters\": n_clusters,\n",
    "        \"silhouette_score\": silhouette,\n",
    "        \"output_file\": filename_json\n",
    "    }]\n",
    "    all_summary.append(summary)\n",
    "    \n",
    "df = pd.DataFrame(all_summary)\n",
    "df.to_csv(os.path.join(OUTPUT_DIR, \"summary_results.csv\"), index=False)\n",
    "\n",
    "print(\"\\n✅ Done! Cluster JSON with year counts saved in:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d13da6c",
   "metadata": {},
   "source": [
    "#  Labelling clusters with LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from krutrim_cloud import KrutrimCloud\n",
    "\n",
    "# === Config ===\n",
    "THRESHOLDS = [0.5, 1, 1.5]\n",
    "OUTPUT_DIR = \"./labeled_cluster_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === API Client ===\n",
    "client = KrutrimCloud(api_key='') #api key \n",
    "MODEL_NAME = \"Llama-3.3-70B-Instruct\"\n",
    "\n",
    "def label_cluster_with_llm(cluster_keywords):\n",
    "    keywords_text = \", \".join(cluster_keywords)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert in materials science. Your task is to assign a single representative keyword or phrase \"\n",
    "                \"to a given list of related materials science research keywords. Choose a concise, specific phrase that \"\n",
    "                \"accurately summarizes the entire cluster. If the keywords are too unrelated or vary widely, return only: None.\\n\\n\"\n",
    "                \"Rules:\\n\"\n",
    "                \"1. Choose a single short phrase or keyword (no more than 4-5 words).\\n\"\n",
    "                \"2. Make it specific and relevant (e.g., 'Mott-Schottky analysis', not 'materials characterization').\\n\"\n",
    "                \"3. Output only the label or 'None' with no additional explanation.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Cluster: {keywords_text}\\n\\nOutput a single label or 'None':\"\n",
    "        },\n",
    "    ]\n",
    "    try:\n",
    "        response = client.chat.completions.create(model=MODEL_NAME, messages=messages)\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API Error: {e}\")\n",
    "        return \"None\"\n",
    "\n",
    "# === Process each threshold file ===\n",
    "for thresh in THRESHOLDS:\n",
    "    input_file = f\"clusters_dt{thresh}_with_year_counts.json\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        clusters = json.load(f)\n",
    "\n",
    "    print(f\"\\n🔍 Processing {input_file}...\")\n",
    "\n",
    "    labeled_output = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for cluster_id, cluster_data in clusters.items():\n",
    "        keywords = cluster_data[\"keywords\"]\n",
    "        years = cluster_data[\"years\"]\n",
    "\n",
    "        label = label_cluster_with_llm(keywords)\n",
    "        if label == \"None\":\n",
    "            continue\n",
    "\n",
    "        for year, count in years.items():\n",
    "            labeled_output[label][str(year)] += count\n",
    "\n",
    "    # Sort inner dicts by year\n",
    "    final_output = {\n",
    "        label: dict(sorted(year_data.items()))\n",
    "        for label, year_data in labeled_output.items()\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    output_file = os.path.join(OUTPUT_DIR, f\"labeled_clusters_dt{thresh}.json\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ Saved labeled results to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1ea61",
   "metadata": {},
   "source": [
    "# Saving precomputed embeddings for these 3 variants of final keywords set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419620ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === save_embeddings.py ===\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "FILE_MAP = {\n",
    "    \"Fine-grained (0.5 threshold, ~9000)\": \"labeled_clusters_dt0.5.json\",\n",
    "    \"Moderate (1.0 threshold, ~3000)\": \"labeled_clusters_dt1.json\",\n",
    "    \"Broad (1.5 threshold, ~1600)\": \"labeled_clusters_dt1.5.json\"\n",
    "}\n",
    "DATA_DIR = \"./labeled_cluster_results\"\n",
    "SAVE_DIR = \"./precomputed_embeddings\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "for label, file in FILE_MAP.items():\n",
    "    file_path = os.path.join(DATA_DIR, file)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cluster = json.load(f)\n",
    "        keywords = list(cluster.keys())\n",
    "\n",
    "    embeddings = model.encode(keywords, show_progress_bar=True)\n",
    "\n",
    "    # Save with label key in filename\n",
    "    base_name = label.split(\"(\")[0].strip().replace(\" \", \"_\").lower()\n",
    "    np.save(os.path.join(SAVE_DIR, f\"{base_name}_embeddings.npy\"), embeddings)\n",
    "    with open(os.path.join(SAVE_DIR, f\"{base_name}_keywords.json\"), \"w\") as f:\n",
    "        json.dump(keywords, f)\n",
    "\n",
    "    print(f\"Saved: {label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
