{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66198, 384)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "print(embeddings.shape)  # Should be (n, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing UMAP: n_components=10, n_neighbors=10, min_dist=0.01 [1/1000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Applying AgglomerativeClustering with distance_threshold=0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define expanded parameter combinations\n",
    "n_components_range = range(10, 101, 10)  # 10, 20, 30, ..., 100\n",
    "n_neighbors_range = range(10, 101, 10)   # 10, 20, 30, ..., 100\n",
    "min_dist_values = [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.65, 0.8, 0.99]\n",
    "distance_thresholds = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "\n",
    "# File to track progress\n",
    "progress_file = \"clustering_progress.json\"\n",
    "results_file = \"clustering_results.csv\"\n",
    "\n",
    "# Generate all parameter combinations\n",
    "umap_params = []\n",
    "for n_comp in n_components_range:\n",
    "    for n_neigh in n_neighbors_range:\n",
    "        for min_dist in min_dist_values:\n",
    "            umap_params.append((n_comp, n_neigh, min_dist))\n",
    "\n",
    "# Load existing results and progress if available\n",
    "results = []\n",
    "completed_params = set()\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    df_results = pd.read_csv(results_file)\n",
    "    results = df_results.to_dict('records')\n",
    "    \n",
    "    # Extract already completed parameter combinations\n",
    "    for row in results:\n",
    "        param_key = (row[\"n_components\"], row[\"n_neighbors\"], row[\"min_dist\"], row[\"distance_threshold\"])\n",
    "        completed_params.add(param_key)\n",
    "    \n",
    "    print(f\"Loaded {len(results)} existing results from {results_file}\")\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    with open(progress_file, 'r') as f:\n",
    "        progress_data = json.load(f)\n",
    "        last_index = progress_data.get('last_index', 0)\n",
    "        print(f\"Resuming from parameter combination index {last_index}\")\n",
    "else:\n",
    "    last_index = 0\n",
    "\n",
    "# Process remaining parameter combinations\n",
    "try:\n",
    "    for param_idx, (n_comp, n_neigh, min_dist) in enumerate(umap_params[last_index:], start=last_index):\n",
    "        print(f\"Processing UMAP: n_components={n_comp}, n_neighbors={n_neigh}, min_dist={min_dist} [{param_idx+1}/{len(umap_params)}]\")\n",
    "        \n",
    "        # Apply UMAP\n",
    "        umap_reducer = umap.UMAP(\n",
    "            n_components=n_comp, \n",
    "            n_neighbors=n_neigh, \n",
    "            min_dist=min_dist, \n",
    "            metric='cosine', \n",
    "            random_state=42\n",
    "        )\n",
    "        umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "        \n",
    "        for dist_thresh in distance_thresholds:\n",
    "            # Skip if this combination has already been processed\n",
    "            param_key = (n_comp, n_neigh, min_dist, dist_thresh)\n",
    "            if param_key in completed_params:\n",
    "                print(f\"  - Skipping AgglomerativeClustering with distance_threshold={dist_thresh} (already processed)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"  - Applying AgglomerativeClustering with distance_threshold={dist_thresh}\")\n",
    "            \n",
    "            # Apply Agglomerative Clustering\n",
    "            cluster_model = AgglomerativeClustering(n_clusters=None, distance_threshold=dist_thresh)\n",
    "            labels = cluster_model.fit_predict(umap_embeddings)\n",
    "            num_clusters = len(set(labels))\n",
    "            \n",
    "            # Compute silhouette score only if there is more than 1 cluster and more than 1 sample\n",
    "            if num_clusters > 1 and len(set(labels)) < len(labels):\n",
    "                try:\n",
    "                    sil_score = silhouette_score(umap_embeddings, labels)\n",
    "                except:\n",
    "                    sil_score = -1  # Error in calculation\n",
    "            else:\n",
    "                sil_score = -1  # Invalid case (all points in one cluster)\n",
    "            \n",
    "            # Save cluster assignments to JSON\n",
    "            clustered_keywords = {}\n",
    "            for keyword, cluster in zip(keywords_2011, labels):\n",
    "                clustered_keywords.setdefault(str(cluster), []).append(keyword)\n",
    "            \n",
    "            filename = f\"clustered_keywords_ncomp{n_comp}_nneigh{n_neigh}_mindist{min_dist}_dthresh{dist_thresh}.json\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(clustered_keywords, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"    - Saved results to {filename}\")\n",
    "            \n",
    "            # Store results in table\n",
    "            results.append({\n",
    "                \"n_components\": n_comp,\n",
    "                \"n_neighbors\": n_neigh,\n",
    "                \"min_dist\": min_dist,\n",
    "                \"distance_threshold\": dist_thresh,\n",
    "                \"num_clusters\": num_clusters,\n",
    "                \"silhouette_score\": sil_score\n",
    "            })\n",
    "            \n",
    "            # Save results incrementally\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results.to_csv(results_file, index=False)\n",
    "            \n",
    "            # Update progress\n",
    "            with open(progress_file, 'w') as f:\n",
    "                json.dump({'last_index': param_idx}, f)\n",
    "            \n",
    "            completed_params.add(param_key)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProcess interrupted by user. Progress has been saved.\")\n",
    "\n",
    "print(f\"\\nClustering completed/paused. Results saved in '{results_file}'.\")\n",
    "\n",
    "# If we have enough results, generate the visualizations\n",
    "if results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Find best parameters\n",
    "    valid_results = df_results[df_results[\"silhouette_score\"] > 0]\n",
    "    if not valid_results.empty:\n",
    "        best_params = valid_results.loc[valid_results[\"silhouette_score\"].idxmax()]\n",
    "        print(f\"\\nBest parameters found so far:\")\n",
    "        print(f\"  n_components: {best_params['n_components']}\")\n",
    "        print(f\"  n_neighbors: {best_params['n_neighbors']}\")\n",
    "        print(f\"  min_dist: {best_params['min_dist']}\")\n",
    "        print(f\"  distance_threshold: {best_params['distance_threshold']}\")\n",
    "        print(f\"  num_clusters: {best_params['num_clusters']}\")\n",
    "        print(f\"  silhouette_score: {best_params['silhouette_score']}\")\n",
    "    \n",
    "    # Create a more readable visualization with subplots\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Group by n_components and create a subplot for each\n",
    "    unique_n_comp = sorted(df_results[\"n_components\"].unique())\n",
    "    num_plots = len(unique_n_comp)\n",
    "    rows = int(np.ceil(num_plots / 3))  # 3 plots per row\n",
    "    \n",
    "    for i, n_comp in enumerate(unique_n_comp):\n",
    "        ax = plt.subplot(rows, 3, i+1)\n",
    "        \n",
    "        comp_data = df_results[df_results[\"n_components\"] == n_comp]\n",
    "        for (n_neigh, min_dist), group in comp_data.groupby([\"n_neighbors\", \"min_dist\"]):\n",
    "            valid_data = group[group[\"silhouette_score\"] > -1]\n",
    "            if not valid_data.empty:\n",
    "                ax.plot(\n",
    "                    valid_data[\"distance_threshold\"], \n",
    "                    valid_data[\"silhouette_score\"], \n",
    "                    marker=\"o\", \n",
    "                    label=f\"n_neigh={n_neigh}, min_dist={min_dist}\"\n",
    "                )\n",
    "        \n",
    "        ax.set_xlabel(\"Distance Threshold\")\n",
    "        ax.set_ylabel(\"Silhouette Score\")\n",
    "        ax.set_title(f\"n_components = {n_comp}\")\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Only show legend for the first subplot to save space\n",
    "        if i == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"silhouette_scores.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    try:\n",
    "        # Create a heatmap for top parameter combinations\n",
    "        top_results = df_results.sort_values(\"silhouette_score\", ascending=False).head(100)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pivot = pd.pivot_table(\n",
    "            top_results, \n",
    "            values=\"silhouette_score\", \n",
    "            index=[\"n_components\", \"n_neighbors\"],\n",
    "            columns=[\"min_dist\", \"distance_threshold\"]\n",
    "        )\n",
    "        plt.imshow(pivot, cmap=\"viridis\")\n",
    "        plt.colorbar(label=\"Silhouette Score\")\n",
    "        plt.title(\"Top 100 Parameter Combinations by Silhouette Score\")\n",
    "        plt.savefig(\"top_parameters_heatmap.png\", dpi=300)\n",
    "    except:\n",
    "        print(\"Could not create heatmap visualization (likely due to sparse data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
